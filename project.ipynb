{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6202e462",
   "metadata": {},
   "source": [
    "### **Neural Networks Project** \n",
    "Sequencer: Deep LSTM for Image Classification\n",
    "###### Vincenzo Guarino - 1742728"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "2e7a1475",
   "metadata": {},
   "source": [
    "### Paper Introduction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b3a56f73",
   "metadata": {},
   "source": [
    "Sequencer: Deep LSTM for Image Classification proposes a novel architecture for image classification, called Sequencer, based on the idea of replacing the self-attention layer presented in the Vision Transformer (ViT) with Long Short-Term Memory (LSTM) networks. \n",
    "\n",
    "This change has the goal to improve the memory efficiency of the architecture while also keeping the ability to learn long-range dependencies.\n",
    "\n",
    "The paper also introduces a two-dimensional version of Sequencer (the one implemented in this project) called Sequencer2D, which uses two bidirectional LSTM to process the vertical horizontal axis of the image patches in parallel to enhance performance and reduce the sequence length."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b69ab59b",
   "metadata": {},
   "source": [
    "#### **Sequencer2D architecture**\n",
    "\n",
    "| ![sequencer architecture](https://i.imgur.com/ZEbPTmD.png)|\n",
    "|:--:| \n",
    "| *Sequencer architecture* | \n",
    "\n",
    "The overall structure of the Sequencer architecture is represented in the image above.\n",
    "\n",
    "As we can see, it takes as input non-overlapping patches of an image which are then processed by the main (repeating) component: the Sequencer2D block, represented here:\n",
    "\n",
    "| ![sequencerblock architecture](https://i.imgur.com/cCQfuz3.png)|\n",
    "|:--:| \n",
    "| *Sequencer2D Block* |\n",
    "\n",
    "The Sequencer2D Block has two sub-components, a MLP layer for channel-mixing and the BiLSTM layer.\n",
    "\n",
    "The MLP layer is derived by the ViT architecture, it is composed by two linear transformations and a GELU activation function in between.\n",
    "\n",
    "The BiLSTM2D Layer is composed as following:\n",
    "\n",
    "| ![bilstm2d architecture](https://i.imgur.com/Zkj1G8g.png)|\n",
    "|:--:| \n",
    "| *BiLSTM2D layer* |\n",
    "\n",
    "The BiLSTM2D layer consists of two plain BiLSTMs: one vertical processeing each column of patches as a sequence and one horizontal doing the same but for the rows. \n",
    "\n",
    "The outputs of both BiLSTMs are then concatenated and processed point-wise by a linear layer to obtain the final output of the layer.\n",
    "\n",
    "Compared to the multi-head attention layer in ViT, the BiLSTM2D layer scales better for high-resolution images, in fact the BiLSTM2D layer has a memory complexity of (WC + HC)/2, compared to h∗(HW)^2 of the multi-head attention, where h is a number of heads the multi-head attention layer, H is the number of tokens in the vertical direction, W is the number of sequences in the horizontal direction, and C is the channel dimension.\n",
    "\n",
    "There is an advantage even on throughput, with the computational complexity of self-attention being O(W^4C), compared to O(WC^2) of the BiLSTM (with W = H for simplicity)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4c428fb",
   "metadata": {},
   "source": [
    "### Code implementation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5885de91",
   "metadata": {},
   "source": [
    "### Part 1: Imports and initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fddb03de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pylight\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, optim\n",
    "\n",
    "torch.set_float32_matmul_precision(\"high\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "156adefb",
   "metadata": {},
   "source": [
    "### Part 2: Hyperparameters definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc3f82f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# I decided to go with a class in order to conveniently organize all the hyperparameters \n",
    "class SequencerParams():\n",
    "\n",
    "    def __init__(self, variant, dataset):\n",
    "        # stage params as described in table 4 of the paper\n",
    "        if variant.lower() == \"xs\":\n",
    "            self.layers = [2, 2, 4, 3]\n",
    "            self.drop_path = 0.1\n",
    "        elif variant.lower() == \"s\":\n",
    "            self.layers = [4, 3, 8, 3]\n",
    "            self.drop_path = 0.1\n",
    "        elif variant.lower() == \"m\":\n",
    "            self.layers = [4, 3, 14, 3]\n",
    "            self.drop_path = 0.2\n",
    "        elif variant.lower() == \"l\":\n",
    "            self.drop_path = 0.4\n",
    "            self.layers = [8, 8, 16, 4]\n",
    "        self.dropout = 0.0\n",
    "        self.mlp_ratio = 3\n",
    "        self.stage_num = 4\n",
    "        self.mixup_alpha = 0.8\n",
    "        self.cutmix_alpha = 1.0\n",
    "        self.label_smoothing = 0.1\n",
    "        self.weight_decay = 0.05\n",
    "        self.cycle_decay = 0.5\n",
    "        self.lr_min = 1e-6\n",
    "        self.random_erasing = 0.25\n",
    "        self.crop_pct = 0.875\n",
    "        self.auto_augment = \"rand-m9-mstd0.5-inc1\"\n",
    "        \n",
    "        if dataset == \"imagenet\":\n",
    "            self.input_size = [3, 224, 224]\n",
    "            self.num_classes = 200\n",
    "            self.dataset_name = \"imagenet\"\n",
    "        elif dataset == \"tiny-imagenet-200\":\n",
    "            self.input_size = [3, 64, 64]\n",
    "            self.num_classes = 200\n",
    "            self.dataset_name = \"tiny-imagenet-200\"\n",
    "        elif dataset == \"cifar10\":\n",
    "            self.input_size = [3, 32, 32]\n",
    "            self.num_classes = 10\n",
    "            self.dataset_name = \"cifar10\"\n",
    "        \n",
    "        # opt and training params\n",
    "        self.epochs = 300\n",
    "        self.cooldown_epochs = 10\n",
    "        self.warmup_epochs = 20\n",
    "        self.batch_size = 256\n",
    "        self.img_size = self.input_size[1]\n",
    "        self.embed_dims = [192, 384, 384, 384]\n",
    "        self.hidden_dims = [48, 96, 96, 96]\n",
    "        self.patch_sizes = [7, 2, 1, 1]\n",
    "        self.base_lr = 2e-3\n",
    "\n",
    "seq_params = SequencerParams(\"S\", \"cifar10\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9c8c6e9e",
   "metadata": {},
   "source": [
    "### Part 3a: Prepare Tiny-Imagenet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e8b0d0d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# code edited from https://github.com/pytorch/vision/issues/6127#issuecomment-1555049003\n",
    "import os\n",
    "import re\n",
    "def create_dir(base_path, classname):\n",
    "    path = os.path.join(base_path, classname)\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "\n",
    "def reorg(filename, base_path, wordmap):\n",
    "    with open(filename) as vals:\n",
    "        for line in vals:\n",
    "            vals = line.split()\n",
    "            imagename = vals[0]\n",
    "            classname = wordmap[vals[1]]\n",
    "            old_path = os.path.join(base_path, 'images', imagename)\n",
    "            new_path = os.path.join(base_path, classname, imagename)\n",
    "            if os.path.exists(old_path):\n",
    "                os.rename(old_path, new_path)\n",
    "    os.rmdir(os.path.join(base_path, 'images'))\n",
    "\n",
    "def prepare_tiny_imagenet(imagenet_path):\n",
    "    if not os.path.exists(os.path.join(imagenet_path, 'val', 'images')):\n",
    "        return\n",
    "\n",
    "    wordmap = {}\n",
    "    words_file = os.path.join(imagenet_path, 'words.txt')\n",
    "    wnids_file = os.path.join(imagenet_path, 'wnids.txt')\n",
    "    with open(words_file) as words, open(wnids_file) as wnids:\n",
    "        for line in wnids:\n",
    "            vals = line.split()\n",
    "            wordmap[vals[0]] = \"\"\n",
    "        for line in words:\n",
    "            vals = line.split()\n",
    "            if vals[0] in wordmap:\n",
    "                single_words = vals[1:]\n",
    "                classname =  re.sub(\",\", \"\", single_words[0])\n",
    "                if len(single_words) >= 2:\n",
    "                    classname += '_' + re.sub(\",\", \"\", single_words[1])\n",
    "                wordmap[vals[0]] = classname\n",
    "                create_dir(os.path.join(imagenet_path, 'val'), classname)\n",
    "                old_train_dir = os.path.join(imagenet_path, 'train', vals[0])\n",
    "                new_train_dir = os.path.join(imagenet_path, 'train', classname)\n",
    "                if os.path.exists(old_train_dir):\n",
    "                    os.rename(old_train_dir, new_train_dir)\n",
    "\n",
    "    val_annotations_file = os.path.join(imagenet_path, 'val', 'val_annotations.txt')\n",
    "    val_dir = os.path.join(imagenet_path, 'val')\n",
    "    reorg(val_annotations_file, val_dir, wordmap)\n",
    "    \n",
    "    train_dir = os.path.join(imagenet_path, \"train\")\n",
    "    classes = os.listdir(train_dir)\n",
    "    for classname in classes:\n",
    "        source_dir = os.path.join(train_dir, classname, \"images\")\n",
    "        dest_dir = os.path.join(train_dir, classname)\n",
    "        for file in os.listdir(source_dir):\n",
    "            source_file = os.path.join(source_dir, file)\n",
    "            dest_file = os.path.join(dest_dir, file)\n",
    "            os.rename(source_file, dest_file)\n",
    "        os.rmdir(source_dir)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "21506096",
   "metadata": {},
   "source": [
    "### Part 3b: Dataset preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22c26d4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from timm.data import create_dataset, create_loader\n",
    "from torch.utils.data import Subset\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset_path = \"./\" + seq_params.dataset_name + \"/\"\n",
    "\n",
    "if seq_params.dataset_name == \"imagenet\":\n",
    "    traindir = os.path.join(dataset_path, 'train')\n",
    "    valdir = os.path.join(dataset_path, 'val')\n",
    "\n",
    "    os.makedirs(traindir, exist_ok=True)\n",
    "    os.makedirs(valdir, exist_ok=True)\n",
    "\n",
    "    # Create the train and val datasets\n",
    "    train_dataset = create_dataset(name=\"\",\n",
    "        root=traindir, split=\"train\", is_training=True,\n",
    "        batch_size=seq_params.batch_size)\n",
    "\n",
    "    val_dataset = create_dataset(name=\"\",\n",
    "        root=valdir, split=\"val\", is_training=False,\n",
    "        batch_size=seq_params.batch_size)\n",
    "        \n",
    "elif seq_params.dataset_name == \"tiny-imagenet-200\":\n",
    "    traindir = os.path.join(dataset_path, 'train')\n",
    "    valdir = os.path.join(dataset_path, 'val')\n",
    "\n",
    "    os.makedirs(traindir, exist_ok=True)\n",
    "    os.makedirs(valdir, exist_ok=True)\n",
    "\n",
    "    prepare_tiny_imagenet(dataset_path)\n",
    "\n",
    "    # Create the train and val datasets\n",
    "    train_dataset = create_dataset(name=\"\",\n",
    "        root=traindir, split=\"train\", is_training=True,\n",
    "        batch_size=seq_params.batch_size)\n",
    "\n",
    "    val_dataset = create_dataset(name=\"\",\n",
    "        root=valdir, split=\"val\", is_training=False,\n",
    "        batch_size=seq_params.batch_size)\n",
    "\n",
    "elif seq_params.dataset_name == \"cifar10\":\n",
    "    train_dataset = create_dataset(name=\"torch/cifar10\",\n",
    "        root=dataset_path, split=\"train\", download=True, is_training=True,\n",
    "        batch_size=seq_params.batch_size)\n",
    "\n",
    "    # Split the train dataset into train and val subsets\n",
    "    train_indices, test_indices, _, _ = train_test_split(\n",
    "        range(len(train_dataset)),\n",
    "        train_dataset.targets,\n",
    "        stratify=train_dataset.targets,\n",
    "        test_size=0.1,\n",
    "    )\n",
    "\n",
    "    train_dataset = Subset(train_dataset, train_indices)\n",
    "    val_dataset = Subset(train_dataset, test_indices)\n",
    "else:\n",
    "    # Raise an error if the dataset name is not valid\n",
    "    raise ValueError(\"Invalid dataset name: {}\".format(seq_params.dataset_name))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ab608a64",
   "metadata": {},
   "source": [
    "### Part 4: Model building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d66bc0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This classes are derived from the Vision Transformer and didn't had any implementation details in the paper,\n",
    "# so I adapted them from the official github implementation\n",
    "\n",
    "class PatchEmbedding(pylight.LightningModule):\n",
    "    def __init__(self, patch_size, embed_dim, flatten=False):\n",
    "        super().__init__()\n",
    "        self.flatten = flatten\n",
    "        self.conv = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        if self.flatten:\n",
    "            x = x.flatten(2).transpose(1, 2) \n",
    "        else:\n",
    "            x = x.permute(0, 2, 3, 1)  # BCHW -> BHWC\n",
    "        return x\n",
    "\n",
    "class DownsamplePatch(pylight.LightningModule):\n",
    "    def __init__(self, input_dim, output_dim, patch_size):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(input_dim, output_dim, kernel_size=patch_size, stride=patch_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.conv(x)\n",
    "        x = x.permute(0, 2, 3, 1)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "abdc84b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchmetrics\n",
    "from timm.scheduler.cosine_lr import CosineLRScheduler\n",
    "from timm.models.layers import DropPath\n",
    "from torchmetrics import Accuracy\n",
    "\n",
    "\n",
    "class BiLSTM2D(pylight.LightningModule):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.rnn_v = torch.nn.LSTM(\n",
    "            input_size, hidden_size, num_layers=1, batch_first=True, bias=True, bidirectional=True)\n",
    "        self.rnn_h = torch.nn.LSTM(\n",
    "            input_size, hidden_size, num_layers=1, batch_first=True, bias=True, bidirectional=True)\n",
    "        self.fc = torch.nn.Linear(4 * hidden_size, input_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, H, W, C = x.shape\n",
    "\n",
    "        v, _ = self.rnn_v(x.permute(0, 2, 1, 3).reshape(-1, H, C))\n",
    "        v = v.reshape(B, W, H, -1).permute(0, 2, 1, 3)\n",
    "        h, _ = self.rnn_h(x.reshape(-1, W, C))\n",
    "        h = h.reshape(B, H, W, -1)\n",
    "        x = torch.cat([v, h], dim=-1)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class Sequencer2DBlock(pylight.LightningModule):\n",
    "    def __init__(self, embed_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.bilstm2d = BiLSTM2D(embed_dim, hidden_dim)\n",
    "        self.drop_path = DropPath(seq_params.drop_path)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "        mlp_in = embed_dim\n",
    "        mlp_out = embed_dim\n",
    "        mlp_hidden = int(embed_dim * seq_params.mlp_ratio)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(mlp_in, mlp_hidden),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(seq_params.dropout),\n",
    "            nn.Linear(mlp_hidden, mlp_out),\n",
    "            nn.Dropout(seq_params.dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        # Token mixing with BiLSTM2D\n",
    "        x = x + self.drop_path(self.bilstm2d(self.norm1(x)))\n",
    "        # Channel mixing with MLP\n",
    "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
    "        return x\n",
    "\n",
    "\n",
    "class Sequencer2D(pylight.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.valid_acc = torchmetrics.Accuracy(task=\"multiclass\", num_classes=seq_params.num_classes)\n",
    "\n",
    "        stage_list = []\n",
    "        for stage in range(seq_params.stage_num):\n",
    "            if stage == 0:\n",
    "                stage_list.append(PatchEmbedding(patch_size=seq_params.patch_sizes[0],\n",
    "                                                 embed_dim=seq_params.embed_dims[0]))\n",
    "            for layer in range(seq_params.layers[stage]):\n",
    "                stage_list.append(Sequencer2DBlock(embed_dim=seq_params.embed_dims[stage],\n",
    "                                                   hidden_dim=seq_params.hidden_dims[stage]))\n",
    "            if stage < len(seq_params.embed_dims) - 1:\n",
    "                stage_list.append(DownsamplePatch(input_dim=seq_params.embed_dims[stage],\n",
    "                                                  output_dim=seq_params.embed_dims[stage+1],\n",
    "                                                  patch_size=seq_params.patch_sizes[stage+1]))\n",
    "        self.stages = nn.Sequential(*stage_list)\n",
    "\n",
    "        self.norm = nn.LayerNorm(seq_params.embed_dims[-1])\n",
    "        self.fc = nn.Linear(seq_params.embed_dims[-1], seq_params.num_classes)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.stages(x)\n",
    "        x = self.norm(x)\n",
    "        # global average pooling\n",
    "        x = x.mean(dim=(1, 2))\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        opt = optim.AdamW(self.parameters(), lr=seq_params.base_lr, weight_decay=seq_params.weight_decay)\n",
    "        scheduler = CosineLRScheduler(opt, t_initial=seq_params.epochs,\n",
    "                                      lr_min=seq_params.lr_min, \n",
    "                                      cycle_decay=seq_params.cycle_decay,\n",
    "                                      warmup_t=seq_params.warmup_epochs,\n",
    "                                      warmup_lr_init=seq_params.lr_min)\n",
    "        seq_params.epochs = scheduler.get_cycle_length() + seq_params.cooldown_epochs\n",
    "        return [opt], [{\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"epoch\"\n",
    "        }]\n",
    "\n",
    "    def lr_scheduler_step(self, scheduler, metric):\n",
    "        scheduler.step(epoch=self.current_epoch)\n",
    "\n",
    "    # self.log logs to tensorboard\n",
    "    def training_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        preds = self(x)\n",
    "        loss = F.cross_entropy(preds, y, label_smoothing = seq_params.label_smoothing)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        return loss\n",
    "\n",
    "    def validation_step(self, batch, _):\n",
    "        x, y = batch\n",
    "        logits = self(x)\n",
    "        val_loss = F.cross_entropy(logits, y)\n",
    "        self.log(\"val_loss\", val_loss, prog_bar=True)\n",
    "        self.valid_acc(logits, y)\n",
    "        self.log('valid_acc', self.valid_acc, prog_bar=True, on_step=True, on_epoch=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c2eb3104",
   "metadata": {},
   "source": [
    "### Part 5: Data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "57e3d22c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from timm.data import FastCollateMixup\n",
    "\n",
    "collate = FastCollateMixup(mixup_alpha=seq_params.mixup_alpha, cutmix_alpha=seq_params.cutmix_alpha,\n",
    "    label_smoothing=seq_params.label_smoothing, num_classes=seq_params.num_classes)\n",
    "\n",
    "train_loader = create_loader(\n",
    "        train_dataset.dataset if seq_params.dataset_name == \"cifar10\" else train_dataset,\n",
    "        input_size=seq_params.input_size,\n",
    "        batch_size=seq_params.batch_size,\n",
    "        is_training=True,\n",
    "        re_prob=seq_params.random_erasing,\n",
    "        re_mode=\"pixel\",\n",
    "        scale=[0.08, 1.0],\n",
    "        ratio=[0.75, 1.33],\n",
    "        auto_augment=seq_params.auto_augment,\n",
    "        interpolation=\"random\",\n",
    "        num_workers=8,\n",
    "        collate_fn=collate,\n",
    "        pin_memory=True\n",
    "    )\n",
    "\n",
    "val_loader = create_loader(\n",
    "        val_dataset.dataset if seq_params.dataset_name == \"cifar10\" else val_dataset,\n",
    "        input_size=seq_params.input_size,\n",
    "        batch_size=seq_params.batch_size,\n",
    "        is_training=False,\n",
    "        interpolation=\"bicubic\",\n",
    "        crop_pct=seq_params.crop_pct,\n",
    "        num_workers=8,\n",
    "        pin_memory=True\n",
    "    )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7441ac61",
   "metadata": {},
   "source": [
    "### Part 6: Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9b05deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_model = Sequencer2D()\n",
    "\n",
    "# Used bf16-mixed precision since it speeds up the training time on my local machine (RTX 3070)\n",
    "trainer = pylight.Trainer(precision=\"bf16-mixed\", max_epochs=seq_params.epochs)\n",
    "trainer.fit(seq_model, train_loader, val_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5033117f",
   "metadata": {},
   "source": [
    "### Part 7: Results\n",
    "\n",
    "The authors of the paper used the ImageNet-1K dataset, which contains 1.281.167 training images with a resolution of 224x224 pixels. However, I could not train the model on such a large dataset with my local machine. \\\n",
    "I attempted to reduce the model size by creating a “XS” version with fewer layers, but it still wasn't enough to complete the ImageNet-1K training in less then 24h of training. Therefore, I decided to use smaller datasets for this project: CIFAR10 and Tiny-Imagenet. \\\n",
    "CIFAR10 has 60.000 images of 32x32 pixels in 10 classes, with 6000 images per class. Tiny-Imagenet has 100.000 images of 64x64 pixels in 200 classes, with 500 images per class. \\\n",
    "Unfortunately, training the Sequencer model with such low-res data probably doesn't fully exploit the LSTM’s ability to capture long-range dependencies, which could explain why after trying both datasets, I was only able to reach an accuracy of about 60% on the validation set. \\\n",
    "I'll report the tensorboard data for both trainings after 300 epochs:\n",
    "\n",
    "CIFAR10:\n",
    "\n",
    "| ![CIFAR10 validation accuracy](https://i.imgur.com/Ho90Shd.png)| ![CIFAR10 validation loss](https://i.imgur.com/LcDK7Fk.png) |\n",
    "|:--:| :--: |\n",
    "| *CIFAR10 validation accuracy* | *CIFAR10 validation loss* |\n",
    "\n",
    "Tiny-Imagenet:\n",
    "\n",
    "| ![Tiny-Imagenet validation accuracy](https://i.imgur.com/VxpnNct.png)| ![Tiny-Imagenet validation loss](https://i.imgur.com/EV7mYys.png) |\n",
    "|:--:| :--: |\n",
    "| *Tiny-Imagenet validation accuracy* | *Tiny-Imagenet validation loss* |\n",
    "\n",
    "I could probably reach an higher accuracy by tweaking the hyperparameters more (I mostly used the values mentioned in the paper), however this was also proven difficult as each training would require hours to complete."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
